{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). This notebook is used to explore a smaller subset with Spark before running on the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import avg, col, concat, count, desc, \\\n",
    "asc, explode, lit, min, max, split, stddev, udf, isnan, when, rank, \\\n",
    "log, sqrt, cbrt, exp\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, \\\n",
    "RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, \\\n",
    "PCA, RegexTokenizer, Tokenizer, StandardScaler, StopWordsRemover, \\\n",
    "StringIndexer, VectorAssembler, MaxAbsScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from time import time\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import random\n",
    "%matplotlib inline\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Clean Dataset\n",
    "\n",
    "### 1.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load from local source as a pilot study\n",
    "path = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.json(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in the dataset: 286500.\n",
      "\n",
      "artist: 58392 missing values.         0 NaN's; 58392 Null's; 0 empty strings.\n",
      "firstName: 8346 missing values.         0 NaN's; 8346 Null's; 0 empty strings.\n",
      "gender: 8346 missing values.         0 NaN's; 8346 Null's; 0 empty strings.\n",
      "lastName: 8346 missing values.         0 NaN's; 8346 Null's; 0 empty strings.\n",
      "length: 58392 missing values.         0 NaN's; 58392 Null's; 0 empty strings.\n",
      "location: 8346 missing values.         0 NaN's; 8346 Null's; 0 empty strings.\n",
      "registration: 8346 missing values.         0 NaN's; 8346 Null's; 0 empty strings.\n",
      "song: 58392 missing values.         0 NaN's; 58392 Null's; 0 empty strings.\n",
      "userAgent: 8346 missing values.         0 NaN's; 8346 Null's; 0 empty strings.\n",
      "userId: 8346 missing values.         0 NaN's; 0 Null's; 8346 empty strings.\n"
     ]
    }
   ],
   "source": [
    "# Examine the number of missing values in each column\n",
    "print(f\"Total number of rows in the dataset: {df.count()}.\\n\")\n",
    "missing_count_total = 0\n",
    "\n",
    "for coln in df.columns:\n",
    "    missing_count = df.filter((isnan(df[coln])) | (df[coln].isNull()) | (df[coln] == \"\")).count()\n",
    "    \n",
    "    if missing_count > 0:\n",
    "        nan_count = df.filter(isnan(df[coln])).count()\n",
    "        null_count = df.filter(df[coln].isNull()).count()\n",
    "        empty_str = df.filter(df[coln] == \"\").count()\n",
    "        print(f\"{coln}: {missing_count} missing values. \\\n",
    "        {nan_count} NaN's; {null_count} Null's; {empty_str} empty strings.\")\n",
    "        missing_count_total += missing_count\n",
    "        \n",
    "if missing_count_total == 0:\n",
    "    print(\"No missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values in `userId` and `sessionId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 286500 rows in the dataset.\n",
      "There are 0 NaN's in either userId or sessionId. These rows are dropped.\n"
     ]
    }
   ],
   "source": [
    "# Remove rows that have NaN in either userId or sessionId\n",
    "num_rows = df.count()\n",
    "print(f\"There are {df.count()} rows in the dataset.\")\n",
    "df = df.dropna(how='any', subset=['userId', 'sessionId'])\n",
    "print(f\"There are {num_rows - df.count()} NaN's in either userId or sessionId. These rows are dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|      |\n",
      "|    10|\n",
      "|   100|\n",
      "|100001|\n",
      "|100002|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the values of useId\n",
    "df.select(['userId']).dropDuplicates().orderBy(df['userId']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `userId` of empty string looks suspicious. To figure out what's going on, I'm going to look at the `page` events for empty `userId`'s in comparison with all `userId`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view to run SQL queries\n",
    "df.createOrReplaceTempView(\"df_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               page|\n",
      "+-------------------+\n",
      "|               Home|\n",
      "|              About|\n",
      "|Submit Registration|\n",
      "|              Login|\n",
      "|           Register|\n",
      "|               Help|\n",
      "|              Error|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# page events of users whose userId IS an empty string\n",
    "spark.sql(\n",
    "    '''\n",
    "    SELECT DISTINCT page\n",
    "    FROM df_table\n",
    "    WHERE userId == \"\"\n",
    "    '''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|            Settings|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|             Upgrade|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# page events of users whose userId is NOT an empty string\n",
    "spark.sql(\n",
    "    '''\n",
    "    SELECT DISTINCT page\n",
    "    FROM df_table\n",
    "    EXCEPT\n",
    "    SELECT DISTINCT page\n",
    "    FROM df_table\n",
    "    WHERE userId == \"\"\n",
    "    '''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `userId` of empty string likely represents user(s) who are in the middle of or prior to sign-in or registration. Therefore, the rows corresponding to empty `userId` are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have empty userId\n",
    "df = df.filter(df['userId'] != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `sessionId` column is numeric, we don't need to worry about empty string problem for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values:\n",
      "    userId: 0 missing values.\n",
      "    sessionId: 0 missing values.\n",
      "    firstName: 0 missing values.\n",
      "    gender: 0 missing values.\n",
      "    lastName: 0 missing values.\n",
      "    location: 0 missing values.\n",
      "    registration: 0 missing values.\n",
      "    userAgent: 0 missing values.\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values in userId and sessionId\n",
    "print(\"Number of missing values:\")\n",
    "for coln in ['userId', 'sessionId', 'firstName', 'gender', 'lastName', 'location', 'registration', 'userAgent']:\n",
    "    missing_count = df.filter((isnan(df[coln])) | (df[coln].isNull()) | (df[coln] == \"\")).count()\n",
    "    print(f\"    {coln}: {missing_count} missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values in `artist`, `length`, `song`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "|            Settings|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# page events of null artist values\n",
    "df.filter(df['artist'].isNull()).select(df['page']).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    page|\n",
      "+--------+\n",
      "|NextSong|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# page events of NOT null artist values\n",
    "df.filter(df['artist'].isNotNull()).select(df['page']).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that only page event \"NextSong\", i.e. playing a song, has corresponding artist information, which makes sense. This should apply for each of the `artist`, `length`, and `song` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When page event is 'NextSong':\n",
      "    artist: 0 missing values.\n",
      "    length: 0 missing values.\n",
      "    song: 0 missing values.\n"
     ]
    }
   ],
   "source": [
    "# Check the number of missing values of artist, length, song columns when page event is \"NextSong\"\n",
    "df_song = df.filter(df['page'] == \"NextSong\")\n",
    "print(\"When page event is 'NextSong':\")\n",
    "for coln in ['artist', 'length', 'song']:\n",
    "    missing_count = df_song.filter((isnan(df[coln])) | (df[coln].isNull()) | (df[coln] == \"\")).count()\n",
    "    print(f\"    {coln}: {missing_count} missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Perform EDA using a small subset of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique userId's to be sampled for exploratory data analysis\n",
    "user_sample_size = 250\n",
    "\n",
    "# Unique userId's from the full dataset\n",
    "users = [row.userId for row in df.select('userId').dropDuplicates().collect()]\n",
    "\n",
    "# Sample userId's\n",
    "if len(users) > user_sample_size:\n",
    "    user_sample = list(np.random.choice(users, size=user_sample_size, replace=False))\n",
    "else:\n",
    "    user_sample = list(np.copy(users))\n",
    "\n",
    "# Create subset of the full dataset\n",
    "df_sub = df.filter(df['userId'].isin(user_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Overview of numerical columns\n",
    "\n",
    "A quick look at descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns (except for sessionId)\n",
    "num_cols = ['itemInSession', 'registration', 'status', 'ts', 'length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+------------------+--------------------+-----------------+\n",
      "|summary|     itemInSession|        registration|            status|                  ts|           length|\n",
      "+-------+------------------+--------------------+------------------+--------------------+-----------------+\n",
      "|  count|            278154|              278154|            278154|              278154|           228108|\n",
      "|   mean|114.89918174824018|1.535358834084427...|209.10321620397335|1.540958915431871...|249.1171819778458|\n",
      "| stddev|  129.851729399489| 3.291321616327586E9|30.151388851328214|1.5068287123306298E9|99.23517921058361|\n",
      "|    min|                 0|       1521380675000|               200|       1538352117000|          0.78322|\n",
      "|    max|              1321|       1543247354000|               404|       1543799476000|       3024.66567|\n",
      "+-------+------------------+--------------------+------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe numerical columns (except for sessionId)\n",
    "df_sub.select(num_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible values of 'status':\n",
      "[307, 404, 200]\n"
     ]
    }
   ],
   "source": [
    "# Possible values of status\n",
    "coln = 'status'\n",
    "print(f\"Possible values of '{coln}':\")\n",
    "print([x[coln] for x in df_sub.select(coln).dropDuplicates().collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`status` values do not have continuous meaning, so I will consider column `status` as a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Overview of non-numerical columns\n",
    "\n",
    "A quick look at the possible categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-numerical columns (except for userId)\n",
    "cat_cols = ['artist', 'auth', 'firstName', 'lastName', 'gender', 'level', 'location',\n",
    "            'method', 'page', 'song', 'userAgent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible values of 'auth':\n",
      "['Cancelled', 'Logged In']\n"
     ]
    }
   ],
   "source": [
    "# Possible values of auth\n",
    "coln = 'auth'\n",
    "print(f\"Possible values of '{coln}':\")\n",
    "print([x[coln] for x in df_sub.select(coln).dropDuplicates().collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible values of 'gender':\n",
      "['F', 'M']\n"
     ]
    }
   ],
   "source": [
    "# Possible values of gender\n",
    "coln = 'gender'\n",
    "print(f\"Possible values of '{coln}':\")\n",
    "print([x[coln] for x in df_sub.select(coln).dropDuplicates().collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible values of 'level':\n",
      "['free', 'paid']\n"
     ]
    }
   ],
   "source": [
    "# Possible values of level\n",
    "coln = 'level'\n",
    "print(f\"Possible values of '{coln}':\")\n",
    "print([x[coln] for x in df_sub.select(coln).dropDuplicates().collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible values of 'method':\n",
      "['PUT', 'GET']\n"
     ]
    }
   ],
   "source": [
    "# Possible values of method\n",
    "coln = 'method'\n",
    "print(f\"Possible values of '{coln}':\")\n",
    "print([x[coln] for x in df_sub.select(coln).dropDuplicates().collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible values of 'page':\n",
      "['Cancel', 'Submit Downgrade', 'Thumbs Down', 'Home', 'Downgrade', 'Roll Advert', 'Logout', 'Save Settings', 'Cancellation Confirmation', 'About', 'Settings', 'Add to Playlist', 'Add Friend', 'NextSong', 'Thumbs Up', 'Help', 'Upgrade', 'Error', 'Submit Upgrade']\n"
     ]
    }
   ],
   "source": [
    "# Possible values of page\n",
    "coln = 'page'\n",
    "print(f\"Possible values of '{coln}':\")\n",
    "print([x[coln] for x in df_sub.select(coln).dropDuplicates().collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible values of 'userAgent':\n",
      "['\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', '\"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:31.0) Gecko/20100101 Firefox/31.0', '\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', '\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36\"', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:31.0) Gecko/20100101 Firefox/31.0', 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:30.0) Gecko/20100101 Firefox/30.0', 'Mozilla/5.0 (Windows NT 6.3; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', 'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)', '\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"']\n"
     ]
    }
   ],
   "source": [
    "# Possible values of userAgent\n",
    "coln = 'userAgent'\n",
    "print(f\"Possible values of '{coln}':\")\n",
    "print([x[coln] for x in df_sub.select(coln).dropDuplicates().collect()[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|status| page|\n",
      "+------+-----+\n",
      "|   404|Error|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A status of 404 corresponds to a page event \"Error\"\n",
    "df_sub.select(['status', 'page']).where(df_sub['status']==404).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Define churn\n",
    "\n",
    "Create a column `Churn` to use as the label for the model. I decided to use the `Cancellation Confirmation` events to define churn, which happen for both paid and free users. Here I define churn as no longer using any music service, either as a free user or as a paid user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_churn_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "df_sub = df_sub.withColumn(\"churned\", flag_churn_event(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.1% users have churned.\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentage of users who churned\n",
    "pctg = df_sub.groupBy('userId').agg({'churned': 'sum'})\\\n",
    "    .select(avg('sum(churned)')).collect()[0]['avg(sum(churned))']\n",
    "print(f\"{round(pctg*100, 1)}% users have churned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate flags of churn\n",
    "windowval = Window.partitionBy(\"userId\").orderBy(asc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# 1 means the user has already churned at this timestamp\n",
    "df_sub = df_sub.withColumn(\"churnPhase\", Fsum('churned').over(windowval))\n",
    "\n",
    "# 1 means the user is a churn user\n",
    "df_sub = df_sub.withColumn(\"churnUser\", max('churned').over(Window.partitionBy(\"userId\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Explore data\n",
    "Analyze the behavior for users who stayed vs users who churned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "In this section, I'm going to build out the features that I think useful to train my model on. First, I'm going to do pilot experiment on a small subset of the full dataset. Then, I will use the code on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
